{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The secound question I was trying to answer in my capstone project was: Predict  the number of people injured in a collision  given a geographic location. Im using for this task neural network separeting the geografical data in one layer(Im using a simplified radial based function created by Stephen Rose) and for the rest of my data Im using just dense layers. To show the result of my work, Im plotting my result as a heatmap as a layer on an API(Googles maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gmaps\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error,accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import initializers\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Layer\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Input, Flatten, Reshape, Conv1D, MaxPool1D, concatenate\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.metrics import binary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my clean data\n",
    "Data_merg = pd.read_csv(\"collisions_merge.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kills are too random to try to predict, so Im adding kills to the injured \n",
    "Data_merg['TOTAL'] = Data_merg['TOTAL INJURED']+ Data_merg['TOTAL KILLED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting my data \n",
    "\n",
    "x = Data_merg[['YEAR', 'MONTH', 'DAY', 'TIME', 'LATITUDE', 'LONGITUDE',\n",
    "                       'season','Events', 'CloudCover','PrecipitationIn',\n",
    "                       'Max.TemperatureF', 'Mean.TemperatureF', 'Min.TemperatureF',\n",
    "                       'Max.Dew.PointF', 'MeanDew.PointF', 'Min.DewpointF', 'Max.Humidity',\n",
    "                       'Mean.Humidity', 'Min.Humidity', 'Max.VisibilityMiles',\n",
    "                       'Mean.VisibilityMiles', 'Min.VisibilityMiles', 'Max.Wind.SpeedMPH',\n",
    "                       'Mean.Wind.SpeedMPH', 'Max.Gust.SpeedMPH','CONTR FACTOR VEHICLE 1',\n",
    "                       'CONTR FACTOR VEHICLE 2', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2',\n",
    "              ]]\n",
    "y = Data_merg['TOTAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding my categorical data and creating my train, test, val sets\n",
    "\n",
    "x_dummy = pd.get_dummies(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dummy, y, test_size=0.3)\n",
    "\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My geografic data\n",
    "\n",
    "x_geo_train, x_geo_test = x_train[['LATITUDE', 'LONGITUDE']], x_test[['LATITUDE', 'LONGITUDE']]\n",
    "x_geo_val, x_geo_test = x_val[['LATITUDE', 'LONGITUDE']], x_test[['LATITUDE', 'LONGITUDE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other factors\n",
    "x_fact_train, x_fact_test = x_train.drop(['LATITUDE', 'LONGITUDE'], axis=1),\\\n",
    "                            x_test.drop(['LATITUDE', 'LONGITUDE'], axis=1)\n",
    "\n",
    "x_fact_val, x_fact_test = x_val.drop(['LATITUDE', 'LONGITUDE'], axis=1),\\\n",
    "                            x_test.drop(['LATITUDE', 'LONGITUDE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF Layer (rosey's function)\n",
    "class RBFLayer(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.centers, self.betas = [None] * 2\n",
    "        super(RBFLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(RBFLayer, self).build(input_shape)\n",
    "        self.centers = self.add_weight(\n",
    "            name='centers',\n",
    "            shape=(self.output_dim, input_shape[1]),\n",
    "            initializer=initializers.he_uniform(),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.betas = self.add_weight(\n",
    "            name='betas',\n",
    "            shape=(self.output_dim,),\n",
    "            initializer=initializers.constant(1.0),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # RBF activation function\n",
    "        # \\varphi = exp[-\\beta * ||x-\\mu||^2]\n",
    "        c = K.expand_dims(self.centers)\n",
    "        h = K.transpose(c - K.transpose(inputs))\n",
    "        return K.exp(-self.betas * K.sum(h ** 2, axis=1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.output_dim\n",
    "    \n",
    "\n",
    "def keras_frac_var_unexplained(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the unexplained variance\n",
    "    SumSq(y - y_hat) / SumSq(y - y_mean)\n",
    "    \"\"\"\n",
    "    return K.sum(K.square(y_true - y_pred)) / K.sum(K.square(y_true - K.mean(y_true)))\n",
    "\n",
    "\n",
    "def keras_r2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Rsq for Keras models\n",
    "    \"\"\"\n",
    "    return 1 - keras_frac_var_unexplained(y_true, y_pred)\n",
    "\n",
    "\n",
    "def plot_decision_boundary(model, x, y, extrapolation=1.2):\n",
    "    x_min, x_max = x[:, 0].min() * extrapolation, x[:, 0].max() * extrapolation\n",
    "    y_min, y_max = x[:, 1].min() * extrapolation, x[:, 1].max() * extrapolation\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    pred = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    pred = pred.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, pred, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 107)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          10800       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 400)          40400       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "rbf_layer_1 (RBFLayer)          (None, 200)          600         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          40100       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 300)          0           rbf_layer_1[0][0]                \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 50)           15050       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            51          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 107,001\n",
      "Trainable params: 107,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# My NN model\n",
    "\n",
    "inputs1 = Input(shape=(x_geo_train.shape[1],))\n",
    "inputs2 = Input(shape=(x_fact_train.shape[1],))\n",
    "\n",
    "rbf_layer = RBFLayer(200)(inputs1) # Lat and Long layer\n",
    "\n",
    "dnn = Dense(100, activation='relu')(inputs2)\n",
    "dnn = Dense(400, activation='relu')(dnn)\n",
    "dnn = Dense(100, activation='relu')(dnn)\n",
    "\n",
    "concat = concatenate([rbf_layer, dnn]) #concat \n",
    "\n",
    "dnn_final = Dense(50, activation='relu')(concat)\n",
    "outputs = Dense(1)(dnn_final)\n",
    "\n",
    "My_NN = Model([inputs1,inputs2], outputs)\n",
    "\n",
    "print(My_NN.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MSE = 0.431641971233568\n"
     ]
    }
   ],
   "source": [
    "# checking my baseline\n",
    "\n",
    "n = len(y_train)\n",
    "base_mse = mean_squared_error(y_train, y_train.mean() * np.ones(n))\n",
    "print(f'Baseline MSE = {base_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile my NN model\n",
    "\n",
    "%%time\n",
    "My_NN.compile('adam', loss='mse', metrics=['mae'])\n",
    "My_NN.fit(\n",
    "    [x_geo_train, x_fact_train], y_train,\n",
    "    validation_data=([x_geo_val, x_fact_val], y_val),\n",
    "    epochs=10000, batch_size=10000,\n",
    "    callbacks=[ModelCheckpoint('NN_collision_merge2.h5', save_best_only=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load my weights\n",
    "# My_NN.load_weights('NN_collision_merge.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot googlemaps\n",
    "\n",
    "gmaps.configure(api_key=\"AIzaSyDzycl8r7PemimeH-bbP_n7dS76keNlfC8\") # MY Google API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# using my prediction as weights on a heatmap \n",
    "\n",
    "weight=x_geo_test.copy()\n",
    "\n",
    "weight['PRED'] =  My_NN.predict([x_geo_test,x_fact_test])\n",
    "weight['PRED'][weight['PRED']<0]=0\n",
    "\n",
    "w = weight.sort_values('PRED', ascending= False)\n",
    "x_geo = w[['LATITUDE','LONGITUDE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4379f91a7aa4d0d850a69586fc1c86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(layout=FigureLayout(height='420px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Number of injuried people 2012-2015 period \n",
    "\n",
    "new_york_coordinates = (40.75, -74.00)\n",
    "\n",
    "fig = gmaps.figure(map_type='HYBRID' ,center=new_york_coordinates, zoom_level=15)\n",
    "fig.add_layer(gmaps.transit_layer())\n",
    "fig.add_layer(gmaps.traffic_layer())\n",
    "heatmap_layer=gmaps.heatmap_layer(x_geo,weights=w['PRED'])\n",
    "heatmap_layer.max_intensity =2\n",
    "\n",
    "heatmap_layer.point_radius = 10\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# average of people injuried 2012-2015\n",
    "\n",
    "dev = w.groupby(['LATITUDE','LONGITUDE']).sum()/\\\n",
    "w.groupby(['LATITUDE','LONGITUDE']).count()\n",
    "\n",
    "dev.reset_index(inplace=True)\n",
    "\n",
    "weight2 = dev[dev['PRED']>0]\n",
    "x_geo2 = weight2[['LATITUDE','LONGITUDE']]\n",
    "weight2['PRED']=weight2['PRED']*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68a85dde69a433a8079363073b923c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Figure(layout=FigureLayout(height='420px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "new_york_coordinates = (40.75, -74.00)\n",
    "fig = gmaps.figure(map_type='HYBRID' ,center=new_york_coordinates, zoom_level=15)\n",
    "fig.add_layer(gmaps.transit_layer())\n",
    "fig.add_layer(gmaps.traffic_layer())\n",
    "heatmap_layer=gmaps.heatmap_layer(x_geo2,weights=weight2['PRED'])\n",
    "heatmap_layer.max_intensity =1\n",
    "heatmap_layer.point_radius = 24\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
